@INPROCEEDINGS{Gapp2024_LLaMA2Med,
  title     = {Multimodal Medical Disease Classification with LLaMA II},
  author    = {Christian Gapp and Elias Tappeiner and Martin Welk and Rainer Schubert},
  booktitle = {Proceedings of Austrian Symposium on AI, Robotics, and Vision 2024},
  year      = {2024},
  doi       = {10.15203/99106-150-2-07},
  url       = {https://doi.org/10.15203/99106-150-2-07},
  note      = {Chest X-ray + report classification with LLaMA II and vision-language fusion (early/late/mixed) + LoRA fine-tuning. Achieves SOTA AUC (0.971) on OpenI. Relevant for multimodal classification. Discusses Early, Late, Mixed fusion pipelines}
}

@INPROCEEDINGS{Moor2023_MedFlamingo,
  title     = {Med-Flamingo: A Multimodal Medical Few-shot Learner},
  author    = {Michael Moor and Qian Huang and Shirley Wu and Michihiro Yasunaga and Yash Dalmia and Jure Leskovec and Cyril Zakka and Eduardo Pontes Reis and Pranav Rajpurkar},
  booktitle = {Proceedings of the 3rd Machine Learning for Health Symposium},
  series    = {Proceedings of Machine Learning Research},
  pages     = {353--367},
  year      = {2023},
  month     = {10 Dec},
  volume    = {225},
  publisher = {PMLR},
  url       = {https://proceedings.mlr.press/v225/moor23a.html},
  note      = {Introduces Med-Flamingo, a few-shot multimodal medical VQA model with interleaved text-image pretraining. Strong in-context learning, human evaluation, but limited by hallucinations and missing pathologies.}
}

@INPROCEEDINGS{Van2024_LargeVLMsMed,
  author    = {Minh-Hao Van and Prateek Verma and Xintao Wu},
  title     = {On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study},
  booktitle = {2024 IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)}, 
  pages     = {172--176},
  year      = {2024},
  doi       = {10.1109/CHASE60773.2024.00029},
  url       = {https://doi.org/10.1109/CHASE60773.2024.00029},
  note      = {Benchmarks VLMs (BiomedCLIP, OpenCLIP, Flamingo, LLaVA, GPT-4) on brain MRI, microscopy, COVID X-rays. Compares zero-/few-shot, prompts, vs CNN baselines. Relevant for VLM eval.}
}

@INPROCEEDINGS{Xia2025_MMedRAG,
  title     = {MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models},
  author    = {Peng Xia and Kangyu Zhu and Haoran Li and Tianze Wang and Weijia Shi and Sheng Wang and Linjun Zhang and James Zou and Huaxiu Yao},
  booktitle = {Proceedings of the 2025 International Conference on Learning Representations (ICLR)},
  year      = {2025},
  url       = {https://proceedings.iclr.cc/paper_files/paper/2025/file/a559a5a8aa5ae6682ced009ad97cdb16-Paper-Conference.pdf},
  note      = {Proposes MMed-RAG with domain-aware retrieval and context selection. Improves factuality and cross-modality alignment on medical VQA/report generation.}
}

@misc{Byra2023_FewShotVLM,
  title     = {Few-shot medical image classification with simple shape and texture text descriptors using vision-language models}, 
  author    = {Michal Byra and Muhammad Febrian Rachmadi and Henrik Skibbe},
  year      = {2023},
  eprint    = {2308.04005},
  archivePrefix = {arXiv},
  primaryClass = {cs.CV},
  doi           = {10.48550/arXiv.2308.04005},
  url       = {https://doi.org/10.48550/arXiv.2308.04005},
  note      = {Uses GPT-4-generated shape/texture descriptors with CLIP for few-shot classification (X-ray, ultrasound). Improves n-shot performance where data is limited.}
}

@INPROCEEDINGS{Wang2022_MedCLIP,
  title     = {MedCLIP: Contrastive Learning from Unpaired Medical Images and Text},
  author    = {Zifeng Wang and Zhenbang Wu and Dinesh Agarwal and Jimeng Sun},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  month     = {Dec},
  pages     = {3876--3887},
  doi       = {10.18653/v1/2022.emnlp-main.256},
  url       = {https://aclanthology.org/2022.emnlp-main.256/},
  note      = {Contrastive pretraining with UMLS/MetaMap to align image, text, and paired data. Strong zero-shot classification/retrieval on chest X-rays, beating ConVIRT/GLoRIA with less data.}
}

@INBOOK{Lei2023_CLIP_Lung,
  title     = {CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction},
  ISBN      = {9783031439902},
  ISSN      = {1611-3349},
  url       = {http://dx.doi.org/10.1007/978-3-031-43990-2_38},
  DOI       = {10.1007/978-3-031-43990-2_38},
  booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2023},
  publisher = {Springer Nature Switzerland},
  author    = {Lei, Yiming and Li, Zilong and Shen, Yan and Zhang, Junping and Shan, Hongming},
  year      = {2023},
  pages     = {403--412},
  note      = {Lung nodule malignancy prediction via CT + textual annotations. Uses channel-wise conditional prompt and knowledge-guided contrastive learning. SOTA on LIDC-IDRI.}
}

@INPROCEEDINGS{Li2023_LLaVA_Med,
  title     = {LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day},
  author    = {Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {28541--28564},
  publisher = {Curran Associates, Inc.},
  volume    = {36},
  year      = {2023},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2023/file/5abcdf8ecdcacba028c6662789194572-Paper-Datasets_and_Benchmarks.pdf},
  note      = {Adapts LLaVA to biomedical domain using PMC-15M. Two-stage curriculum (concept alignment + instruction tuning). Strong biomedical VQA with low cost.}
}

@ARTICLE{Haq2025_Advancements_MMML_Radiology,
  title   = {Advancements in Medical Radiology Through Multimodal Machine Learning: A Comprehensive Overview},
  author  = {Imran Ul Haq and Mustafa Mhamed and Mohammed Al-Harbi and Hamid Osman and Zuhal Y. Hamd and Zhe Liu},
  journal = {Bioengineering (Basel)},
  volume  = {12},
  number  = {5},
  pages   = {477},
  year    = {2025},
  doi     = {10.3390/bioengineering12050477},
  url     = {https://doi.org/10.3390/bioengineering12050477},
  note={Survey of multimodal ML in radiology. Covers fusion (early/late/joint), representation learning, cross-modal retrieval. Applications: classification, VQA. Notes future directions incl. time-series, beyond bimodal.}
}

@ARTICLE{Seki2025_ZeroShotECG,
  title   = {Assessing the performance of zero-shot visual question answering in multimodal large language models for 12-lead ECG image interpretation},
  author  = {Seki, Tomohisa and Kawazoe, Yoshimasa and Ito, Hiromasa and Akagi, Yu and Takiguchi, Toru and Ohe, Kazuhiko},
  journal = {Frontiers in Cardiovascular Medicine},
  volume  = {12},
  pages   = {1458289},
  year    = {2025},
  doi     = {10.3389/fcvm.2025.1458289},
  url     = {https://doi.org/10.3389/fcvm.2025.1458289},
  note    = {Tests ViLT, Gemini Pro Vision, ChatGPT on ECG VQA. Models biased toward “normal”; ChatGPT best but still hallucinates. Highlights evaluation and structured data needs.}
}

@ARTICLE{Sharma2021_MedFuseNet,
  title   = {MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain},
  author  = {Sharma, Dhruv and Purushotham, Sanjay and Reddy, Chandan K.},
  journal = {Scientific Reports},
  volume  = {11},
  number  = {1},
  pages   = {19826},
  year    = {2021},
  doi     = {10.1038/s41598-021-98390-1},
  url     = {https://doi.org/10.1038/s41598-021-98390-1},
  note    = {Attention-based multimodal fusion for medical VQA. Achieves SOTA with interpretable attention visualizations.}
}

@ARTICLE{Lu2024CoDVQA,
  title   = {Collaborative Modality Fusion for Mitigating Language Bias in Visual Question Answering},
  author  = {Lu, Qiwen and Chen, Shengbo and Zhu, Xiaoke},
  journal = {Journal of Imaging},
  volume  = {10},
  number  = {3},
  pages   = {56},
  year    = {2024},
  doi     = {10.3390/jimaging10030056},
  url     = {https://doi.org/10.3390/jimaging10030056},
  note    = {CoD-VQA: collaborative modality fusion reducing language/vision bias in VQA by allowing rich modalities to assist deprived ones.}
}

@ARTICLE{Abdullakutty2024_MultiModalHistopathology,
  title   = {Histopathology in focus: a review on explainable multi-modal approaches for breast cancer diagnosis},
  author  = {Abdullakutty, Faseela and Akbari, Younes and Al-Maadeed, Somaya and Bouridane, Ahmed and Talaat, Iman M. and Hamoudi, Rifat},
  journal = {Frontiers in Medicine},
  volume  = {11},
  pages   = {1450103},
  year    = {2024},
  doi     = {10.3389/fmed.2024.1450103},
  url     = {https://doi.org/10.3389/fmed.2024.1450103},
  note    = {Review of multi-modal breast cancer histopathology methods. Compares unimodal vs multimodal (histopath, clinical, genomic). Discusses fusion strategies, imbalance issues, clinical applicability.}
}

@ARTICLE{Sun2023_MultimodalReview,
  title   = {A scoping review on multimodal deep learning in biomedical images and texts},
  author  = {Sun, Zhaoyi and Lin, Mingquan and Zhu, Qingqing and Xie, Qianqian and Wang, Fei and Lu, Zhiyong and Peng, Yifan},
  journal = {Journal of Biomedical Informatics},
  volume  = {146},
  pages   = {104482},
  year    = {2023},
  doi     = {10.1016/j.jbi.2023.104482},
  url     = {https://doi.org/10.1016/j.jbi.2023.104482},
  note    = {Review of multimodal deep learning in biomedicine (images + text). Covers report generation, VQA, retrieval, CAD. Notes challenges: imbalance, data scarcity, limited modalities, fairness/interpretability. Suggests dataset creation, knowledge integration, stronger evals.}
}

@article{Rajpurkar2017_CheXNet,
  author       = {Pranav Rajpurkar and
                  Jeremy Irvin and
                  Kaylie Zhu and
                  Brandon Yang and
                  Hershel Mehta and
                  Tony Duan and
                  Daisy Yi Ding and
                  Aarti Bagul and
                  Curtis P. Langlotz and
                  Katie S. Shpanskaya and
                  Matthew P. Lungren and
                  Andrew Y. Ng},
  title        = {CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with
                  Deep Learning},
  journal      = {CoRR},
  volume       = {abs/1711.05225},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.05225},
  eprinttype    = {arXiv},
  eprint       = {1711.05225},
  timestamp    = {Fri, 26 Nov 2021 17:17:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-05225.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Lyu2022_ViT_AD,
author = {Lyu, Yanjun and Yu, Xiaowei and Zhu, Dajiang and Zhang, Lu},
title = {Classification of Alzheimer's Disease via Vision Transformer: Classification of Alzheimer's Disease via Vision Transformer},
year = {2022},
isbn = {9781450396318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529190.3534754},
doi = {10.1145/3529190.3534754},
abstract = {Deep models are powerful in capturing the complex and non-linear relationship buried in brain imaging data. However, the huge number of parameters in deep models can easily overfit given limited imaging data samples. In this work, we proposed a cross-domain transfer learning method to solve the insufficient data problem in brain imaging domain by leveraging the knowledge learned in natural image domain. Specifically, we employed ViT as the backbone and firstly pretrained it using ImageNet-21K dataset and then transferred to the brain imaging dataset. A slice-wise convolution embedding method was developed to improve the standard patch operation in vanilla ViT. Our method was evaluated based on AD/CN classification task. We also conducted extensive experiments to compare the transfer performance with different transfer strategies, models, and sample size. The results suggest that the proposed method can effectively transfer the knowledge learned in natural image domain to brain imaging area and may provide a promising way to take advantages of the pretrained model in data-intensive applications. Moreover, the proposed cross-domain transfer learning method can obtain comparable classification performance compared to most recent studies.},
booktitle = {Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {463–468},
numpages = {6},
location = {Corfu, Greece},
series = {PETRA '22}
}