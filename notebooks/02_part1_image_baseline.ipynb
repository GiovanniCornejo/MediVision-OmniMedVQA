{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Image Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from src.data import load_omnimed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Load the Base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = load_omnimed_dataset()\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "# Check for image overlap\n",
    "print(\"Overlap train-test:\", len(set(train_df['image_path']) & set(test_df['image_path'])))\n",
    "print(\"Overlap train-val:\", len(set(train_df['image_path']) & set(val_df['image_path'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Create Image-Only Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_df(df):\n",
    "    # Extract the text of the correct option using gt_label\n",
    "    df = df.copy()\n",
    "    df['gt_text'] = df.apply(lambda row: row[row['gt_label']], axis=1)\n",
    "    \n",
    "    # Keep only image_path and gt_text for the image-only baseline\n",
    "    return df[['image_path', 'gt_text']]\n",
    "\n",
    "train_img_df = create_image_df(train_df)\n",
    "val_img_df = create_image_df(val_df)\n",
    "test_img_df = create_image_df(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Build the Label Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = pd.concat([train_img_df, val_img_df, test_img_df])['gt_text'].unique()\n",
    "all_labels = sorted(all_labels)\n",
    "label_to_idx = {label: i for i, label in enumerate(all_labels)}\n",
    "idx_to_label = {i: label for label, i in label_to_idx.items()}\n",
    "\n",
    "# Map text to integer labels\n",
    "for df in [train_img_df, val_img_df, test_img_df]:\n",
    "    df['label_idx'] = df['gt_text'].map(label_to_idx)\n",
    "\n",
    "# Verify label distribution\n",
    "print(\"Number of unique labels:\", len(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Image Dataset Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Define Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms (includes augmentation)\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),               # Resize image\n",
    "    transforms.RandomHorizontalFlip(),                # Augment: flip\n",
    "    transforms.RandomRotation(10),            # Augment: slight rotation\n",
    "    transforms.ToTensor(),                            # Convert PIL image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard ImageNet normalization\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation / Test transforms (no augmentation)\n",
    "val_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Create Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniMedImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx]['image_path']\n",
    "        label = self.df.iloc[idx]['label_idx']\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OmniMedImageDataset(train_img_df, transform=train_image_transform)\n",
    "val_dataset = OmniMedImageDataset(val_img_df, transform=val_image_transform)\n",
    "test_dataset = OmniMedImageDataset(test_img_df, transform=val_image_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch_size\n",
    "# TODO: Add to config.py as constant with optional override via environment variable\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Validation batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
