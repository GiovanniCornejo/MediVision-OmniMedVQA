{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Reproduce Existing Multimodal Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import load_omnimed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = load_omnimed_dataset()\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "# Check for image overlap\n",
    "print(\"Overlap train-test:\", len(set(train_df['image_path']) & set(test_df['image_path'])))\n",
    "print(\"Overlap train-val:\", len(set(train_df['image_path']) & set(val_df['image_path'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_omnimed_dataframe(df, include_answer=True):\n",
    "    \"\"\"\n",
    "    Given OmniMedVQA DataFrame, prepare text + image pairs for multimodal training.\n",
    "    Assumes df already has 'image_path'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Label comes from gt_answer\n",
    "    df['label'] = df['gt_answer']\n",
    "\n",
    "    # Build text input\n",
    "    if include_answer:\n",
    "        df['text_input'] = df.apply(\n",
    "            lambda row: f\"Question: {row['question']}\\nAnswer: {row['gt_answer']}\",\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        df['text_input'] = df['question'].apply(lambda q: f\"Question: {q}\\nAnswer:\")\n",
    "\n",
    "    return df[['image_path', 'text_input', 'label']]\n",
    "\n",
    "train_ready = prepare_omnimed_dataframe(train_df, include_answer=True)\n",
    "val_ready   = prepare_omnimed_dataframe(val_df,   include_answer=True)\n",
    "test_ready  = prepare_omnimed_dataframe(test_df,  include_answer=False)\n",
    "\n",
    "print(train_ready.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    cross_attn_every_n_layers=1\n",
    ")\n",
    "\n",
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# from torchvision import transforms\n",
    "# from open_flamingo import create_model_and_transforms\n",
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "# # ---- Dummy image ----\n",
    "# import requests\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "\n",
    "\n",
    "# image = Image.open(\"E:\\MediVision-Flare25\\data\\OmniMedVQA\\Images\\ACRIMA\\Im002_ACRIMA.png\").convert(\"RGB\")\n",
    "# # image_tensor shape: (C,H,W)\n",
    "# image_tensor = image_processor(image)  # shape (3,H,W)\n",
    "\n",
    "# # Add batch, T_img, F dimensions\n",
    "# image_tensor = image_tensor.unsqueeze(0).unsqueeze(1).unsqueeze(2)  \n",
    "# # shape: (1, 1, 1, 3, H, W)\n",
    "\n",
    "\n",
    "# # ---- Text prompt ----\n",
    "# prompt = \"ACRIMA\"\n",
    "\n",
    "# # ---- Tokenize ----\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # ---- Forward pass ----\n",
    "# with torch.no_grad():\n",
    "#     out = model.generate(\n",
    "#         vision_x=image_tensor,   # add time dim\n",
    "#         lang_x=inputs[\"input_ids\"],\n",
    "#         max_new_tokens=20\n",
    "#     )\n",
    "\n",
    "# print(\"Generated:\", tokenizer.decode(out[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class OmniMedDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_processor, tokenizer, include_answer=True, image_root=\"data/OmniMedVQA/Images\"):\n",
    "        self.df = dataframe\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.include_answer = include_answer\n",
    "        self.image_root = Path(image_root)  # ensure consistent paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Fix the image path\n",
    "        image_path = Path(row[\"image_path\"])\n",
    "\n",
    "        # Remove redundant \"Images/\" if present\n",
    "        parts = image_path.parts\n",
    "        if parts[0] == \"Images\":\n",
    "            image_path = Path(*parts[1:])  # remove leading \"Images\"\n",
    "\n",
    "        # Join with root\n",
    "        image_path = (self.image_root / image_path).resolve()\n",
    "\n",
    "        if not image_path.exists():\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "        # Load & preprocess image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = self.image_processor(image).unsqueeze(0).unsqueeze(0)  # (1,1,3,H,W)\n",
    "\n",
    "        # Build text prompt\n",
    "        if self.include_answer:\n",
    "            text = f\"Prompt: {row['text_input']}\\nAnswer: {row['label']}\"\n",
    "        else:\n",
    "            text = f\"Prompt: {row['text_input']}\\nAnswer:\"\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "        return {\n",
    "            \"vision_x\": image_tensor,\n",
    "            \"lang_x\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": row[\"label\"]\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    vision_x = torch.stack([item[\"vision_x\"] for item in batch])\n",
    "    \n",
    "    # pad lang_x sequences\n",
    "    lang_x = pad_sequence([item[\"lang_x\"] for item in batch],\n",
    "                          batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch],\n",
    "                                  batch_first=True, padding_value=0)\n",
    "    \n",
    "    labels = pad_sequence([item[\"lang_x\"] for item in batch],  # causal LM\n",
    "                          batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\n",
    "        \"vision_x\": vision_x,\n",
    "        \"lang_x\": lang_x,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "\n",
    "# # Take a random sample of 500 rows (adjust number as needed)\n",
    "# train_ready_small = train_ready.sample(n=500, random_state=42)\n",
    "# val_ready_small   = val_ready.sample(n=100, random_state=42)\n",
    "\n",
    "# # Rebuild datasets using sampled data\n",
    "# train_dataset = OmniMedDataset(train_ready_small, image_processor, tokenizer, include_answer=True)\n",
    "# val_dataset   = OmniMedDataset(val_ready_small,   image_processor, tokenizer, include_answer=True)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "# val_loader   = DataLoader(val_dataset,   batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "train_ready_debug = train_ready.sample(n=20, random_state=42)\n",
    "train_dataset_debug = OmniMedDataset(train_ready_debug, image_processor, tokenizer, include_answer=True)\n",
    "train_loader = DataLoader(train_dataset_debug, batch_size=1, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "def apply_lora_to_flamingo(model, config, adapter_name=\"default\"):\n",
    "    for block in model.lang_encoder.transformer.blocks:\n",
    "        # Decoder layer attention\n",
    "        if hasattr(block, \"decoder_layer\") and hasattr(block.decoder_layer, \"attn\"):\n",
    "            block.decoder_layer.attn.Wqkv = get_peft_model(\n",
    "                block.decoder_layer.attn.Wqkv, config, adapter_name=adapter_name\n",
    "            )\n",
    "        # Cross-attention layer\n",
    "        if hasattr(block, \"gated_cross_attn_layer\") and hasattr(block.gated_cross_attn_layer, \"attn\"):\n",
    "            attn = block.gated_cross_attn_layer.attn\n",
    "            attn.to_q = get_peft_model(attn.to_q, config, adapter_name=adapter_name)\n",
    "            attn.to_kv = get_peft_model(attn.to_kv, config, adapter_name=adapter_name)\n",
    "\n",
    "    # Old decoder blocks\n",
    "    for block in model.lang_encoder.old_decoder_blocks:\n",
    "        block.attn.Wqkv = get_peft_model(block.attn.Wqkv, config, adapter_name=adapter_name)\n",
    "\n",
    "    # Other gated cross-attn layers\n",
    "    for block in model.lang_encoder.gated_cross_attn_layers:\n",
    "        attn = block.attn\n",
    "        attn.to_q = get_peft_model(attn.to_q, config, adapter_name=adapter_name)\n",
    "        attn.to_kv = get_peft_model(attn.to_kv, config, adapter_name=adapter_name)\n",
    "\n",
    "    print(\"✅ LoRA applied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(3):  # demo: 3 epochs\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to GPU\n",
    "        vision_x = batch[\"vision_x\"].to(device, non_blocking=True)\n",
    "        lang_x = batch[\"lang_x\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "        # Mixed precision forward pass\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(\n",
    "                vision_x=vision_x,\n",
    "                lang_x=lang_x,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=lang_x\n",
    "            )\n",
    "            loss = out.loss\n",
    "\n",
    "        # Backward with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Free unused VRAM\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Step {i}, loss={loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch} finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        generated = model.generate(\n",
    "            vision_x=batch[\"vision_x\"],\n",
    "            lang_x=batch[\"lang_x\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_new_tokens=30\n",
    "        )\n",
    "        print(tokenizer.decode(generated[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "6. Key Points from Moor et al. (Med-Flamingo)\n",
    "\n",
    "Initialize with pretrained Flamingo weights (you already did ✅).\n",
    "\n",
    "Freeze most parameters, adapt only vision-language cross-attn and LM layers (LoRA ✅).\n",
    "\n",
    "Format QA prompts as you already built: \"Question: ... Answer:\".\n",
    "\n",
    "Evaluate with accuracy, BLEU, ROUGE on medical QA tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)          # should say 2.0.1+cu118\n",
    "print(torch.cuda.is_available())  # should be True\n",
    "print(torch.version.cuda)         # should be 11.8\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
