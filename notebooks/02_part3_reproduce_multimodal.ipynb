{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Reproduce Existing Multimodal Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import load_omnimed_dataset\n",
    "\n",
    "# Sets whether or not training loss and epochs will be printed\n",
    "debug_prints = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = load_omnimed_dataset()\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_omnimed_dataframe(df, include_answer=True):\n",
    "    \"\"\"\n",
    "    Given OmniMedVQA DataFrame, prepare text + image pairs for multimodal training.\n",
    "    Assumes df already has 'image_path'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Label comes from gt_answer\n",
    "    df['label'] = df['gt_answer']\n",
    "\n",
    "    # Build text input\n",
    "    if include_answer:\n",
    "        df['text_input'] = df.apply(\n",
    "            lambda row: f\"Question: {row['question']}\\nAnswer: {row['gt_answer']}\",\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        df['text_input'] = df['question'].apply(lambda q: f\"Question: {q}\\nAnswer:\")\n",
    "\n",
    "    return df[['image_path', 'text_input', 'label']]\n",
    "\n",
    "train_ready = prepare_omnimed_dataframe(train_df, include_answer=True)\n",
    "val_ready   = prepare_omnimed_dataframe(val_df,   include_answer=True)\n",
    "test_ready  = prepare_omnimed_dataframe(test_df,  include_answer=False)\n",
    "\n",
    "print(train_ready.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "    cross_attn_every_n_layers=1\n",
    ")\n",
    "\n",
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class OmniMedDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_processor, tokenizer, include_answer=True, image_root=\"data/OmniMedVQA/Images\"):\n",
    "        self.df = dataframe\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.include_answer = include_answer\n",
    "        self.image_root = Path(image_root)  # ensure consistent paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Fix the image path\n",
    "        image_path = Path(row[\"image_path\"])\n",
    "\n",
    "        # Remove redundant \"Images/\" if present\n",
    "        parts = image_path.parts\n",
    "        if parts[0] == \"Images\":\n",
    "            image_path = Path(*parts[1:])  # remove leading \"Images\"\n",
    "\n",
    "        # Join with root\n",
    "        image_path = (self.image_root / image_path).resolve()\n",
    "\n",
    "        if not image_path.exists():\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "        # Load & preprocess image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = self.image_processor(image).unsqueeze(0).unsqueeze(0)  # (1,1,3,H,W)\n",
    "\n",
    "        # Build text prompt\n",
    "        if self.include_answer:\n",
    "            text = f\"Prompt: {row['text_input']}\\nAnswer: {row['label']}\"\n",
    "        else:\n",
    "            text = f\"Prompt: {row['text_input']}\\nAnswer:\"\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "        return {\n",
    "            \"vision_x\": image_tensor,\n",
    "            \"lang_x\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": row[\"label\"]\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn_train(batch):\n",
    "    \"\"\"Right padding for training (loss computation).\"\"\"\n",
    "    vision_x = torch.stack([item[\"vision_x\"] for item in batch])\n",
    "\n",
    "    lang_x = pad_sequence([item[\"lang_x\"] for item in batch],\n",
    "                          batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch],\n",
    "                                  batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = lang_x.clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # ignore pad in loss\n",
    "\n",
    "    return {\n",
    "        \"vision_x\": vision_x,\n",
    "        \"lang_x\": lang_x,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_fn_eval(batch):\n",
    "    \"\"\"Left padding for generation (MosaicGPT requirement).\"\"\"\n",
    "    vision_x = torch.stack([item[\"vision_x\"] for item in batch])\n",
    "\n",
    "    # pad_sequence by default left pads if we flip the sequences first\n",
    "    seqs = [item[\"lang_x\"].flip(0) for item in batch]  # reverse each seq\n",
    "    lang_x = pad_sequence(seqs, batch_first=True,\n",
    "                          padding_value=tokenizer.pad_token_id)\n",
    "    lang_x = lang_x.flip(1)  # flip back so text is in the right place\n",
    "\n",
    "    attn = [item[\"attention_mask\"].flip(0) for item in batch]\n",
    "    attention_mask = pad_sequence(attn, batch_first=True, padding_value=0)\n",
    "    attention_mask = attention_mask.flip(1)\n",
    "\n",
    "    labels = lang_x.clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\n",
    "        \"vision_x\": vision_x,\n",
    "        \"lang_x\": lang_x,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_ready = train_ready.sample(n=100)\n",
    "train_dataset = OmniMedDataset(train_ready, image_processor, tokenizer, include_answer=True)\n",
    "\n",
    "\n",
    "val_ready = val_ready.sample(n=50)  # pick 10 samples for debugging\n",
    "val_dataset = OmniMedDataset(val_ready, image_processor, tokenizer, include_answer=True)\n",
    "\n",
    "test_ready = test_ready.sample(n=20)  \n",
    "test_dataset = OmniMedDataset(test_ready, image_processor, tokenizer, include_answer=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True, collate_fn=collate_fn_train)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn_eval)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "def apply_lora_to_flamingo(model, config, adapter_name=\"default\"):\n",
    "    for block in model.lang_encoder.transformer.blocks:\n",
    "        # Decoder layer attention\n",
    "        if hasattr(block, \"decoder_layer\") and hasattr(block.decoder_layer, \"attn\"):\n",
    "            block.decoder_layer.attn.Wqkv = get_peft_model(\n",
    "                block.decoder_layer.attn.Wqkv, config, adapter_name=adapter_name\n",
    "            )\n",
    "        # Cross-attention layer\n",
    "        if hasattr(block, \"gated_cross_attn_layer\") and hasattr(block.gated_cross_attn_layer, \"attn\"):\n",
    "            attn = block.gated_cross_attn_layer.attn\n",
    "            attn.to_q = get_peft_model(attn.to_q, config, adapter_name=adapter_name)\n",
    "            attn.to_kv = get_peft_model(attn.to_kv, config, adapter_name=adapter_name)\n",
    "\n",
    "    # Old decoder blocks\n",
    "    for block in model.lang_encoder.old_decoder_blocks:\n",
    "        block.attn.Wqkv = get_peft_model(block.attn.Wqkv, config, adapter_name=adapter_name)\n",
    "\n",
    "    # Other gated cross-attn layers\n",
    "    for block in model.lang_encoder.gated_cross_attn_layers:\n",
    "        attn = block.attn\n",
    "        attn.to_q = get_peft_model(attn.to_q, config, adapter_name=adapter_name)\n",
    "        attn.to_kv = get_peft_model(attn.to_kv, config, adapter_name=adapter_name)\n",
    "\n",
    "    print(\"LoRA applied successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "num_epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(\n",
    "            vision_x=batch[\"vision_x\"],\n",
    "            lang_x=batch[\"lang_x\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"lang_x\"]\n",
    "        )\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Step {i}, loss={loss:.4f}\")\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}: avg_train_loss={avg_train_loss:.4f}\")\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(\n",
    "                vision_x=batch[\"vision_x\"],\n",
    "                lang_x=batch[\"lang_x\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"lang_x\"]\n",
    "            )\n",
    "            total_val_loss += out.loss.item()\n",
    "\n",
    "            # Generate predictions\n",
    "            generated = model.generate(\n",
    "                vision_x=batch[\"vision_x\"],\n",
    "                lang_x=batch[\"lang_x\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_new_tokens=30\n",
    "            )\n",
    "            decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n",
    "            all_preds.extend(decoded_preds)\n",
    "\n",
    "            labels = batch[\"labels\"]\n",
    "            decoded_labels = [tokenizer.decode(l[l != -100], skip_special_tokens=True) for l in labels]\n",
    "            all_labels.extend(decoded_labels)\n",
    "\n",
    "            print(f\"Epoch {epoch}, Step {i}, val_loss={out.loss.item():.4f}\")\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch}: val_loss={avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Generate text\n",
    "        generated = model.generate(\n",
    "            vision_x=batch[\"vision_x\"],\n",
    "            lang_x=batch[\"lang_x\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_new_tokens=30\n",
    "        )\n",
    "\n",
    "        # Decode the predictions\n",
    "        decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n",
    "        all_preds.extend(decoded_preds)\n",
    "\n",
    "        # Decode the labels (ground-truth)\n",
    "        labels = batch[\"labels\"]\n",
    "        decoded_labels = [tokenizer.decode(l[l != -100], skip_special_tokens=True) for l in labels]  # ignore padding\n",
    "        all_labels.extend(decoded_labels)\n",
    "\n",
    "        # Optional: print for inspection\n",
    "        for pred, label in zip(decoded_preds, decoded_labels):\n",
    "            print(\"Pred:\", pred)\n",
    "            print(\"GT  :\", label)\n",
    "            print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\bAnswer\\b$\", \"\", text)        # remove dangling \"Answer\"\n",
    "    text = text.replace(\"\\nAnswer\", \"\").strip()    # remove embedded newlines\n",
    "    return text\n",
    "\n",
    "# --- Extract answers ---\n",
    "y_true = [clean_text(l.split(\"Answer:\")[-1]) for l in all_labels]\n",
    "y_pred = [clean_text(p.split(\"Answer:\")[-1]) for p in all_preds]\n",
    "\n",
    "print(\"y_true:\", y_true)\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "# --- Token-level partial matching ---\n",
    "def token_overlap_scores(gt, pred):\n",
    "    gt_tokens = set(gt.split())\n",
    "    pred_tokens = set(pred.split())\n",
    "\n",
    "    if not gt_tokens and not pred_tokens:\n",
    "        return 1, 1, 1  # perfect match if both empty\n",
    "    if not gt_tokens:\n",
    "        return 0, 1, 0  # only prediction present\n",
    "    if not pred_tokens:\n",
    "        return 1, 0, 0  # only ground truth present\n",
    "\n",
    "    tp = len(gt_tokens & pred_tokens)\n",
    "    fp = len(pred_tokens - gt_tokens)\n",
    "    fn = len(gt_tokens - pred_tokens)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# --- Compute scores across all examples ---\n",
    "precisions, recalls, f1s = [], [], []\n",
    "matches = []\n",
    "\n",
    "for gt, pred in zip(y_true, y_pred):\n",
    "    precision, recall, f1 = token_overlap_scores(gt, pred)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    matches.append(1 if gt == pred else 0)\n",
    "\n",
    "accuracy = sum(matches) / len(matches)\n",
    "precision = sum(precisions) / len(precisions)\n",
    "recall = sum(recalls) / len(recalls)\n",
    "f1 = sum(f1s) / len(f1s)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "6. Key Points from Moor et al. (Med-Flamingo)\n",
    "\n",
    "Initialize with pretrained Flamingo weights\n",
    "\n",
    "Freeze most parameters, adapt only vision-language cross-attn and LM layers (LoRA).\n",
    "\n",
    "Format QA prompts as you already built: \"Question: ... Answer:\".\n",
    "\n",
    "Evaluate with accuracy, BLEU, ROUGE on medical QA tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
