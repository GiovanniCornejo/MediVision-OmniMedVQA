{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multimodal Medical Classification: OmniMedVQA Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook performs **data exploration** on the OmniMedVQA dataset: loading the data, ensuring schema consistency, and analyzing the distribution of question types and modalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Initialize Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dir = \"./data/OmniMedVQA/QA_information/Open-access\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Visualize the Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Inspect Schema Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all JSON files\n",
    "json_files = [os.path.join(qa_dir, f) for f in os.listdir(qa_dir) if f.endswith(\".json\")]\n",
    "\n",
    "# Store schema (column names) of each file\n",
    "schema_dict = {}\n",
    "\n",
    "# Load JSON file and store its columns\n",
    "for f in json_files:\n",
    "    try:\n",
    "        df = pd.read_json(f)\n",
    "        schema_dict[os.path.basename(f)] = set(df.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {f}: {e}\")\n",
    "\n",
    "# Display schemas for each file\n",
    "for fname, cols in schema_dict.items():\n",
    "    print(f\"\\n{fname} ({len(cols)} columns):\")\n",
    "    print(sorted(cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Compare to Reference Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequency of each schema across all JSON files\n",
    "schemas = [tuple(sorted(cols)) for cols in schema_dict.values()]\n",
    "\n",
    "# Find most common schema (reference schema) used in the dataset\n",
    "reference_schema, _ = Counter(schemas).most_common(1)[0]\n",
    "print(\"\\nReference schema:\", reference_schema)\n",
    "\n",
    "# Compare each file's schema to the reference schema\n",
    "for fname, cols in schema_dict.items():\n",
    "    extra = cols - set(reference_schema)\n",
    "    missing = set(reference_schema) - cols\n",
    "    if extra or missing:\n",
    "        print(f\"\\nWARNING: {fname}\")\n",
    "        if extra:\n",
    "            print(\"  Extra columns:\", extra)\n",
    "        if missing:\n",
    "            print(\"  Missing columns:\", missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Fix Schema Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all JSON files to fix schema issues\n",
    "for f in json_files:\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Track if any modifications were made\n",
    "    modified = False\n",
    "\n",
    "    # Fix \"modality\" to \"modality_type\"\n",
    "    for entry in data:\n",
    "        if \"modality\" in entry:\n",
    "            entry[\"modality_type\"] = entry.pop(\"modality\")\n",
    "            modified = True\n",
    "    \n",
    "    # Save corrected JSON back to file\n",
    "    if modified:\n",
    "        print(f\"Fixed schema in {os.path.basename(f)}\")\n",
    "        with open(f, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Note on Schema Inconsistency\n",
    "\n",
    "While inspecting the JSON files, we found that `Chest CT Scan.json` contains a single entry using the key `modality` instead of `modality_type`.  \n",
    "\n",
    "We automatically correct this entry so that `modality` is renamed to `modality_type` for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Loading the Unified Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point at local JSON files\n",
    "dataset = load_dataset(\"json\", data_files=json_files, split=\"train\")\n",
    "df: pd.DataFrame =dataset.to_pandas() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Count Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_qa = len(df)\n",
    "print(f\"Number of QA items: {total_qa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_images = df['image_path'].nunique()\n",
    "print(f\"Number of unique images: {unique_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = df['dataset'].nunique()\n",
    "print(f\"Number of datasets represented: {num_datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Question Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of QA items per question type\n",
    "question_type_counts = df['question_type'].value_counts()\n",
    "print(\"QA items per question type:\\n\", question_type_counts)\n",
    "\n",
    "# Visualize with a bar plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(\n",
    "    x=question_type_counts.index,\n",
    "    y=question_type_counts.values,\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Number of QA items\")\n",
    "plt.xlabel(\"Question Type\")\n",
    "plt.title(\"Distribution of Question Types in OmniMedVQA\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Note on Question Type Distribution\n",
    "\n",
    "The dataset is heavily dominated by Disease Diagnosis (55,387 items), followed by Anatomy Identification (16,448) and Modality Recognition (11,565).\n",
    "\n",
    "Less common types such as Other Biological Attributes (3,498) and Lesion Grading (2,098) may require special attention during modeling to avoid underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Ground Truth Answers per Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qtype in df['question_type'].unique():\n",
    "    answers = df[df['question_type'] == qtype]['gt_answer'].value_counts()\n",
    "    top_answers = answers.head(10)\n",
    "    bottom_answers = answers.tail(10)\n",
    "    print(f\"\\nTop 10 answers for question type: {qtype}\")\n",
    "    print(top_answers)\n",
    "    print(f\"\\nBottom 10 answers for question type: {qtype}\")\n",
    "    print(bottom_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Note on Answer Variability and Long-Tail Effects\n",
    "\n",
    "Some question types are heavily skewed toward a few frequent answers:\n",
    "\n",
    "- Disease Diagnosis: `No` and `No, It's normal.` account for ~7,400 QA items.\n",
    "- Modality Recognition: `MRI` and `CT` dominate.\n",
    "\n",
    "Some question types also contains answers that appear very rarely (sometimes only once). For example:\n",
    "\n",
    "- Modality Recognition: \"Histopathology.\" appears 8 times.\n",
    "- Disease Diagnosis: \"Fundus neoplasm\" appears once.\n",
    "\n",
    "This sparsity could make supervised learning on rare classes challenging and may require targeted strategies like oversampling or class weighting.\n",
    "\n",
    "Some semantically identical answers differ in punctuation, capitalization, or minor wording, e.g.:\n",
    "\n",
    "- `x_ray.` vs `X-ray`\n",
    "- `Dermoscopic imaging` vs `Dermoscopy` vs `Dermoscopy.`\n",
    "- `Fundus photography` vs `fundus photography.` vs `fundus photography`\n",
    "\n",
    "Preprocessing steps such as lowercasing, stripping punctuation, and mapping variants to canonical forms may be beneficial. Despite the long-tail and answer variability, all major modalities and question types are represented, which is promising for building a generalizable multimodal model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Dataset-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_counts = df['dataset'].value_counts()\n",
    "print(\"\\nNumber of QA items per dataset:\\n\", dataset_counts)\n",
    "\n",
    "print(\"\\nTop 5 most represented datasets:\")\n",
    "print(dataset_counts.head())\n",
    "\n",
    "print(\"\\nBottom 5 least represented datasets:\")\n",
    "print(dataset_counts.tail())\n",
    "\n",
    "# Identify top 5 and bottom 5 datasets\n",
    "top5 = dataset_counts.head(5).index\n",
    "bottom5 = dataset_counts.tail(5).index\n",
    "\n",
    "# Assign colors\n",
    "colors = []\n",
    "for ds in dataset_counts.index:\n",
    "    if ds in top5:\n",
    "        colors.append(\"green\")\n",
    "    elif ds in bottom5:\n",
    "        colors.append(\"red\")\n",
    "    else:\n",
    "        colors.append(\"lightgray\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(\n",
    "    x=dataset_counts.index,\n",
    "    y=dataset_counts.values,\n",
    "    palette=colors,\n",
    "    hue=dataset_counts.index,\n",
    "    legend=False\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Number of QA items\")\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.title(\"QA Items per Dataset in OmniMedVQA\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Note on Dataset Imbalance\n",
    "\n",
    "While RadImageNet alone contributes 56,697 QA items (>60% of the total), several datasets at the bottom (e.g., Pulmonary Chest MC with 38 items) are very small. This imbalance in datasets isn't necessarily an issue as long as all modalities are adequately represented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Modality Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_counts = df['modality_type'].value_counts()\n",
    "print(\"Number of unique modalities:\", df['modality_type'].nunique())\n",
    "print(\"\\nNumber of QA items per modality:\\n\", modality_counts)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(list(modality_counts.values), labels=list(modality_counts.index), autopct=\"%1.1f%%\", startangle=140)\n",
    "plt.title(\"Distribution of Modalities in OmniMedVQA\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Note on Modality Distribution\n",
    "\n",
    "The OmniMedVQA dataset includes 8 distinct modalities. While MR (Magnetic Resonance Imaging) dominates with ~35.8% of QA items, followed by CT (~17.8%) and Ultrasound (~12.3%), the less frequent modalities such as OCT (5.2%), Fundus Photography (6.1%), and Microscopy Images (7.5%) still have a substantial number of QA items (4,646â€“5,680), which should be sufficient for model training.\n",
    "\n",
    "Although there is a skew toward MR and CT, all clinically relevant modalities are represented, reducing the risk that models will completely ignore underrepresented modalities. However, care may still be needed to ensure that rare modalities are weighted during training or evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
