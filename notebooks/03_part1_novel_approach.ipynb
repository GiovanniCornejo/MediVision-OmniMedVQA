{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Novel Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from tqdm.notebook import tqdm # Progress bars\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from src.data import load_omnimed_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Load the Base Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = load_omnimed_dataset()\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "# Check for image overlap\n",
    "print(\"Overlap train-test:\", len(set(train_df['image_path']) & set(test_df['image_path'])))\n",
    "print(\"Overlap train-val:\", len(set(train_df['image_path']) & set(val_df['image_path'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## OmniMed Dataset Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Define Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms (includes augmentation)\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),                 # Resize image\n",
    "    models.ResNet18_Weights.DEFAULT.transforms()  # Use ResNet18 default transforms\n",
    "])\n",
    "\n",
    "# Validation / Test transforms (no augmentation)\n",
    "val_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    models.ResNet18_Weights.DEFAULT.transforms()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Define Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Create Novel Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniMedNovelDataset(Dataset):\n",
    "    def __init__(self, df, image_transform=None, tokenizer=None, max_length=100):\n",
    "        self.df = df\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_map = {\"option_A\": 0, \"option_B\": 1, \"option_C\": 2, \"option_D\": 3}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # ---- Image ----\n",
    "        image = Image.open(row['image_path']).convert(\"RGB\")\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        # ---- Text ----\n",
    "        # For example: \"Question: What abnormality is present? Options: A. X B. Y C. Z D. W\"\n",
    "        option_labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "        options_text = \" \".join(\n",
    "            f\"{label}. {row[f'option_{label}']}\" \n",
    "            for label in option_labels \n",
    "            if row[f'option_{label}'] is not None\n",
    "        )\n",
    "\n",
    "        text_input = f\"Question: {row['question']} Options: {options_text}\"\n",
    "\n",
    "        if self.tokenizer:\n",
    "            tokens = self.tokenizer(text_input, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "            input_ids = tokens.input_ids.squeeze(0)\n",
    "            attention_mask = tokens.attention_mask.squeeze(0)\n",
    "        else:\n",
    "            input_ids, attention_mask = None, None\n",
    "\n",
    "        label = self.label_map[row['gt_label']]\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = OmniMedNovelDataset(train_df, image_transform=train_image_transform, tokenizer=tokenizer)\n",
    "val_dataset = OmniMedNovelDataset(val_df, image_transform=val_image_transform, tokenizer=tokenizer)\n",
    "test_dataset = OmniMedNovelDataset(test_df, image_transform=val_image_transform, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch_size\n",
    "# TODO: Add to config.py as constant with optional override\n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "print(\"Validation batches:\", len(val_loader))\n",
    "print(\"Test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for i, batch in enumerate(train_loader):\n",
    "    images = batch['image']\n",
    "    labels = batch['label']\n",
    "    print(i, images.shape, labels.shape)\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGatedMultimodalClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention-gated multimodal classifier.\n",
    "    Emphasizes image features over text features via a learnable gating mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fusion_dim=512, num_classes=4, init_image_bias=0.7):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ----- Vision Encoder -----\n",
    "        # Pretrained ResNet18\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.vision_encoder = nn.Sequential(*list(resnet.children())[:-1])  # output: (batch, 512, 1, 1)\n",
    "        self.vision_proj = nn.Linear(512, fusion_dim)\n",
    "        \n",
    "        # ----- Text Encoder -----\n",
    "        # Simple embedding + mean pooling (could swap for a transformer if desired)\n",
    "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, fusion_dim)\n",
    "        \n",
    "        # ----- Attention-Gated Fusion -----\n",
    "        # Gate: learnable parameter between 0-1 for weighting image vs text\n",
    "        self.gate_param = nn.Parameter(torch.tensor(init_image_bias))\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: (batch, 3, H, W)\n",
    "            input_ids: (batch, seq_len)\n",
    "        Returns:\n",
    "            logits: (batch, num_classes)\n",
    "            gate: image/text gate value\n",
    "        \"\"\"\n",
    "        # --- Image features ---\n",
    "        x_img = self.vision_encoder(images)          # (batch, 512, 1, 1)\n",
    "        x_img = x_img.view(x_img.size(0), -1)       # (batch, 512)\n",
    "        x_img = self.relu(self.vision_proj(x_img))  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Text features ---\n",
    "        if input_ids is not None:\n",
    "            text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            x_text = text_outputs.last_hidden_state.mean(dim=1)\n",
    "            x_text = self.relu(self.text_proj(x_text))\n",
    "        else:\n",
    "            x_text = torch.zeros_like(x_img)\n",
    "        \n",
    "        # --- Attention-gated fusion ---\n",
    "        gate = torch.sigmoid(self.gate_param)  # scalar between 0-1\n",
    "        x = gate * x_img + (1 - gate) * x_text\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits, gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = AttentionGatedMultimodalClassifier(\n",
    "    fusion_dim=512,\n",
    "    num_classes=4,\n",
    "    init_image_bias=0.7\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss, Optimizer, Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# TODO: Add to config.py as constant with optional override\n",
    "num_epochs = 1\n",
    "\n",
    "best_val_acc = 0.0\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "best_model_path = os.path.join(\"models\", \"novel_model.pth\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n=== Epoch {epoch+1}/{num_epochs} ===\")\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, gate = model(images, input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = correct_train / total_train\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Vaidation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits, _ = model(images, input_ids)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            val_loss += criterion(logits, labels).item()\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct_val / total_val\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Saved best model with val_acc={best_val_acc:.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model before testing\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "print(f\"Loaded best model from {best_model_path}\")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, _ = model(images, input_ids)\n",
    "\n",
    "        # Predictions\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(y_true == y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(f\"Test Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Test Precision (macro): {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Test Recall (macro):    {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "print(f\"Test F1 (macro):        {f1_score(y_true, y_pred, average='macro'):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
