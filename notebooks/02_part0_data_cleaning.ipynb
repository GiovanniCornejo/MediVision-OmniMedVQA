{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# OmniMedVQA Data Cleaning for Disease Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os, re\n",
    "\n",
    "from src.data import load_omnimed_dataset\n",
    "from src.config import OPTION_COLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Mapping Ground Truth Answers to Options (`gt_label`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine all splits\n",
    "train_df, val_df, test_df = load_omnimed_dataset()\n",
    "full_df = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
    "\n",
    "# Example usage:\n",
    "row = full_df.iloc[1]\n",
    "print(\"Correct option key:\", row[\"gt_label\"])\n",
    "print(\"Correct answer text:\", row[row[\"gt_label\"]])\n",
    "\n",
    "# Columns relevant for cleaning\n",
    "cols_to_clean = ['question_id', 'question', 'gt_label', 'option_A', 'option_B', 'option_C', 'option_D']\n",
    "full_df = full_df[cols_to_clean].copy()\n",
    "\n",
    "print(full_df.head())\n",
    "print(f\"After mapping: {len(full_df)} samples remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicate options in a row\n",
    "def has_duplicate_options(row, cols):\n",
    "    opts = [row[col] for col in cols]\n",
    "    # Drop None/NaN values\n",
    "    opts = [o for o in opts if pd.notna(o)]\n",
    "    return len(opts) != len(set(opts))\n",
    "\n",
    "# Apply to full_df\n",
    "full_df[\"duplicate_options\"] = full_df.apply(lambda r: has_duplicate_options(r, OPTION_COLS), axis=1)\n",
    "\n",
    "# How many rows have duplicates?\n",
    "num_duplicates = full_df[\"duplicate_options\"].sum()\n",
    "print(f\"Number of rows with duplicate options: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Step 0: Export Answer and Option Distributions\n",
    "\n",
    "Rerun this cell to see progress. i.e., for part 1 below run all cells and then rerun this one and you should see most labels have been simplified into \"No Finding\" or \"Abnormal (unspecified)\" or \"Inconclusive\".\n",
    "\n",
    "This should make it easier to keep track of what labels remain to be simplified/modified in the .csv files.\n",
    "\n",
    "Note: Inconclusive labels only appear in non-ground truth options (so you will see them in all_unique_options.csv only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for exploratory outputs\n",
    "export_dir = os.path.join(\"data\", \"exploration_outputs\")\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Count of unique gt_answers\n",
    "answer_counts = full_df.apply(lambda r: r[r[\"gt_label\"]], axis=1).value_counts()\n",
    "answer_counts_file = os.path.join(export_dir, \"gt_answer_counts.csv\")\n",
    "answer_counts.to_csv(answer_counts_file, header=True)\n",
    "print(f\"Exported gt_answer counts to {answer_counts_file}\")\n",
    "\n",
    "# Export all unique options\n",
    "all_options = pd.concat([full_df[col] for col in OPTION_COLS])\n",
    "\n",
    "# Count unique options\n",
    "option_counts = all_options.value_counts()\n",
    "\n",
    "# Export to CSV\n",
    "option_counts_file = os.path.join(export_dir, \"all_unique_options.csv\")\n",
    "option_counts.to_csv(option_counts_file, header=[\"count\"])\n",
    "print(f\"Exported all unique options to {option_counts_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_counts.head(50))  # Top 50 most frequent\n",
    "print(len(answer_counts))      # Total number of unique answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to check what kinds of questions are being asked for a given answer\n",
    "target_answer = None\n",
    "mask = (\n",
    "    (full_df['option_A'] == target_answer) | \n",
    "    (full_df['option_B'] == target_answer) | \n",
    "    (full_df['option_C'] == target_answer) | \n",
    "    (full_df['option_D'] == target_answer) \n",
    ")\n",
    "\n",
    "matching_questions = full_df.loc[mask, 'question']\n",
    "print(matching_questions.value_counts().head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to check what the original answers were for a given question\n",
    "target_question_id = \"ACRIMA_0088\"\n",
    "\n",
    "# Filter the row by question_id\n",
    "row = full_df[full_df['question_id'] == target_question_id].iloc[0]\n",
    "\n",
    "# Show the original options\n",
    "print(\"Original options:\")\n",
    "for col in [\"option_A\", \"option_B\", \"option_C\", \"option_D\"]:\n",
    "    print(f\"{col}: {row[col]}\")\n",
    "\n",
    "# Show which option was the ground truth\n",
    "print(\"\\nGround truth option key:\", row[\"gt_label\"])\n",
    "print(\"Ground truth answer text:\", row[row[\"gt_label\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_answers = answer_counts[answer_counts <= 50]\n",
    "print(rare_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Part 1: Converting \"No\" answers to No Finding and \"Yes\" answers to specific labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Strip quotes, lowercase, remove trailing periods/spaces\n",
    "def normalize_text(s):\n",
    "    return str(s).strip().lower().strip('\"').rstrip(\".\")\n",
    "\n",
    "# Helper function: Apply a single-label mapping\n",
    "def apply_single_label_mapping(df, option_cols, mapping_csv_path, unified_label):\n",
    "    mapping_df = pd.read_csv(mapping_csv_path)\n",
    "    mapping_dict = {normalize_text(raw): unified_label for raw in mapping_df[\"raw_label\"]}\n",
    "\n",
    "    for col in option_cols:\n",
    "        df[col] = df[col].apply(lambda x: mapping_dict.get(normalize_text(x), x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Converting Negative Answers to No Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply No Finding mapping\n",
    "# full_df = apply_single_label_mapping(\n",
    "#     full_df,\n",
    "#     option_cols,\n",
    "#     \"data/label_mappings/no_finding_map.csv\",\n",
    "#     \"No Finding\"\n",
    "# )\n",
    "\n",
    "# Check replacements\n",
    "for col in OPTION_COLS:\n",
    "    no_finding_count = (full_df[col] == \"No Finding\").sum()\n",
    "    print(f\"{col}: {no_finding_count} entries replaced with 'No Finding'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Removing Inconclusive/Uncertain Labels\n",
    "\n",
    "Only if the ground truth answer is inconclusive/uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply Inconclusive mapping\n",
    "# full_df = apply_single_label_mapping(\n",
    "#     full_df,\n",
    "#     OPTION_COLS,\n",
    "#     \"data/label_mappings/inconclusive_map.csv\",\n",
    "#     \"Inconclusive\"\n",
    "# )\n",
    "\n",
    "# Check replacements\n",
    "for col in OPTION_COLS:\n",
    "    inconclusive_count = (full_df[col] == \"Inconclusive\").sum()\n",
    "    print(f\"{col}: {inconclusive_count} entries replaced with 'Inconclusive'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Converting Generic Yes/Abnormal Answers to Abnormal (unspecified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply Abnormal (unspecified) mapping\n",
    "# full_df = apply_single_label_mapping(\n",
    "#     full_df,\n",
    "#     OPTION_COLS,\n",
    "#     \"data/label_mappings/yes_finding_map.csv\",\n",
    "#     \"Abnormal (unspecified)\"\n",
    "# )\n",
    "\n",
    "# Check replacements\n",
    "for col in OPTION_COLS:\n",
    "    abnormal_count = (full_df[col] == \"Abnormal (unspecified)\").sum()\n",
    "    print(f\"{col}: {abnormal_count} entries replaced with 'Abnormal (unspecified)'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Checking for Duplicates in Answer Choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to full_df\n",
    "full_df[\"duplicate_options\"] = full_df.apply(lambda r: has_duplicate_options(r, OPTION_COLS), axis=1)\n",
    "\n",
    "# How many rows have duplicates?\n",
    "num_duplicates = full_df[\"duplicate_options\"].sum()\n",
    "print(f\"Number of rows with duplicate options: {num_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_duplicates(row, option_cols):\n",
    "    # Save GT text\n",
    "    gt_key = row[\"gt_label\"]\n",
    "    gt_text = row[gt_key]\n",
    "\n",
    "    # Collect original options\n",
    "    opts = [row[col] for col in option_cols]\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = {}\n",
    "    for i, opt in enumerate(opts):\n",
    "        if opt not in seen:\n",
    "            seen[opt] = i\n",
    "    unique_opts = list(seen.keys())\n",
    "\n",
    "    # Ensure GT text is in the unique options\n",
    "    if gt_text not in unique_opts:\n",
    "        unique_opts.append(gt_text)\n",
    "\n",
    "    # Pad/truncate back to fixed length\n",
    "    while len(unique_opts) < len(option_cols):\n",
    "        unique_opts.append(None)\n",
    "    unique_opts = unique_opts[:len(option_cols)]\n",
    "\n",
    "    # Assign back options\n",
    "    for col, val in zip(option_cols, unique_opts):\n",
    "        row[col] = val\n",
    "\n",
    "    # Recompute gt_label so it matches the correct column\n",
    "    for col in option_cols:\n",
    "        if row[col] == gt_text:\n",
    "            row[\"gt_label\"] = col\n",
    "            break\n",
    "\n",
    "    return row\n",
    "\n",
    "full_df = full_df.apply(reduce_duplicates, option_cols=OPTION_COLS, axis=1)\n",
    "\n",
    "# Verify no invalid GT\n",
    "def is_gt_valid(row):\n",
    "    gt_col = row[\"gt_label\"]\n",
    "    return (row[gt_col] is not None)\n",
    "\n",
    "num_invalid = (~full_df.apply(is_gt_valid, axis=1)).sum()\n",
    "print(\"Number of rows with invalid ground truth:\", num_invalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Part 2: Removing punctuation/grammar/useless words in answer choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full grammar_trim with simplified features ---\n",
    "def grammar_trim(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    \n",
    "    s = s.strip()\n",
    "\n",
    "    # --- Special cases for abnormalities ---\n",
    "    if re.search(r\"significant abnormality\", s, flags=re.I):\n",
    "        return \"Abnormality\"\n",
    "    if re.search(r\"minor abnormality|insignificant\", s, flags=re.I):\n",
    "        return \"Slight abnormality\"\n",
    "    if re.search(r\"possible abnormality|potential abnormality\", s, flags=re.I):\n",
    "        return \"Possible abnormality\"\n",
    "    if re.search(r\"multiple abnormalities|abnormalities are seen\", s, flags=re.I):\n",
    "        return \"Abnormalities present\"\n",
    "    # --- Preserve 'Inconclusive' as-is ---\n",
    "    if re.search(r\"inconclusive\", s, flags=re.I):\n",
    "        return \"Inconclusive\"\n",
    "    if re.search(r\"inconclusive|does not provide enough information|too low quality|too blurry|cannot determine\", s, flags=re.I):\n",
    "        return \"Can't tell abnormality\"\n",
    "\n",
    "    # --- Remove redundant phrases ---\n",
    "    redundant_phrases = [\n",
    "        r\"^no,? the image .*\",           \n",
    "        r\"^yes,? the image .*\",          \n",
    "        r\"lungs will be affected\",       \n",
    "        r\"\\bthe abnormality shown in this image is\\b\",\n",
    "        r\"\\bthis image indicates\\b\",\n",
    "        r\"\\bthis image shows\\b\",\n",
    "        r\"\\bthis image displays\\b\",\n",
    "        r\"\\bthe diagnosis is\\b\",\n",
    "        r\"\\bthe findings in this image are\\b\",\n",
    "        r\"\\bthere is a\\b\",\n",
    "        r\"\\bthere are\\b\",\n",
    "    ]\n",
    "    for pat in redundant_phrases:\n",
    "        s = re.sub(pat, \"\", s, flags=re.I).strip()\n",
    "\n",
    "    # --- Remove trailing \"in the provided image\" ---\n",
    "    s = re.sub(r\"\\s*,?\\s*in the provided image\\.?$\", \"\", s, flags=re.I)\n",
    "\n",
    "    # --- Head patterns ---\n",
    "    head_patterns = [\n",
    "        r\"^the head appears (.+)\",\n",
    "        r\"^the head is (.+)\",\n",
    "    ]\n",
    "    for pat in head_patterns:\n",
    "        match = re.match(pat, s, flags=re.I)\n",
    "        if match:\n",
    "            s = match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    # --- Tail patterns ---\n",
    "    tail_patterns = [\n",
    "        r\"^yes, the tail appears to be (.+)\",\n",
    "        r\"^no, the tail appears to be (.+)\",\n",
    "    ]\n",
    "    for pat in tail_patterns:\n",
    "        match = re.match(pat, s, flags=re.I)\n",
    "        if match:\n",
    "            s = match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    # --- Vacuole patterns ---\n",
    "    vacuole_patterns = [\n",
    "        r\"^yes, the vacuole (is|appears to be) (.+)\",\n",
    "        r\"^no, the vacuole (is|shows signs of|appears to be) (.+)\"\n",
    "    ]\n",
    "    for pat in vacuole_patterns:\n",
    "        match = re.match(pat, s, flags=re.I)\n",
    "        if match:\n",
    "            s = match.group(2).strip()\n",
    "            break\n",
    "\n",
    "    # --- Lungs patterns ---\n",
    "    lungs_patterns = [\n",
    "        r\"^(.+?)\\.?\\s*lungs will be affected\\.?\",  \n",
    "        r\"^the lungs (in the image )?(are|appear|show|showing) (.+)\", \n",
    "        r\"^the image (shows|depicts|displays) lungs? (.+)\"\n",
    "    ]\n",
    "    for pat in lungs_patterns:\n",
    "        match = re.match(pat, s, flags=re.I)\n",
    "        if match:\n",
    "            desc = match.groups()[-1].strip()\n",
    "            # Special cases for explicit lung disease\n",
    "            if re.search(r\"fibrotic|stiff\", desc, flags=re.I):\n",
    "                s = \"Lung disease\"\n",
    "            else:\n",
    "                s = re.sub(\n",
    "                    r\"^(signs of |showing |show |appears to be |appears |in the image )\",\n",
    "                    \"\",\n",
    "                    desc,\n",
    "                    flags=re.I\n",
    "                ).strip()\n",
    "            break\n",
    "\n",
    "    # --- Retina / Fundus image filters ---\n",
    "    retina_patterns = [\n",
    "        r\"^The presence of multiple abnormalities is evident in this image\\.$\",\n",
    "        r\"^In this image, there are no apparent abnormalities\\.?.*$\",\n",
    "        r\"^The abnormalities in this image are consistent with (.+)\\.$\",\n",
    "        r\"^There is a conspicuous abnormality in (.+?) in this image\\.$\",\n",
    "        r\"^This image displays (.+)$\",\n",
    "        r\"^There is a clear indication of (.+?) in this image\\.$\",\n",
    "        r\"^The image shows (.+)$\",\n",
    "        r\"^This image shows (.+)$\",\n",
    "        r\"^The anomalies visible in this image suggest (.+)$\",\n",
    "        r\"^The anomalies in this image indicate the presence of (.+)$\",\n",
    "        r\"^The abnormality in this image is consistent with (.+)$\",\n",
    "    ]\n",
    "    for pat in retina_patterns:\n",
    "        match = re.match(pat, s, flags=re.I)\n",
    "        if match:\n",
    "            if \"multiple abnormalities\" in s.lower():\n",
    "                s = \"Abnormalities present\"\n",
    "            elif \"no apparent abnormalities\" in s.lower():\n",
    "                s = \"No Abnormalities\"\n",
    "            else:\n",
    "                s = match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    # --- Acronyms ---\n",
    "    s = re.sub(\n",
    "        r\"\\((?:proliferative diabetic retinopathy|age-related macular degeneration)\\)\",\n",
    "        lambda m: m.group(0)[1:-1],\n",
    "        s,\n",
    "        flags=re.I\n",
    "    )\n",
    "\n",
    "    # --- Remove trailing generic image phrases ---\n",
    "    s = re.sub(\n",
    "        r\"\\s*(,?\\s*(in|on)( the)? (provided )?image)\\s*\\.?$\",\n",
    "        \"\",\n",
    "        s,\n",
    "        flags=re.I\n",
    "    )\n",
    "\n",
    "    # --- General cleanup ---\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = s.rstrip(\".\")\n",
    "    if s:\n",
    "        s = s[0].upper() + s[1:]\n",
    "    return s\n",
    "\n",
    "\n",
    "# --- Apply to a copy of full_df ---\n",
    "option_cols = [\"option_A\", \"option_B\", \"option_C\", \"option_D\"]\n",
    "trimmed_df = full_df.copy()  # <-- new variable\n",
    "\n",
    "# Keep a list of mappings for export\n",
    "mappings = []\n",
    "\n",
    "for col in option_cols:\n",
    "    def apply_trim(x):\n",
    "        if isinstance(x, str):\n",
    "            cleaned = grammar_trim(x)\n",
    "            if cleaned != x:\n",
    "                mappings.append((x, cleaned))\n",
    "            return cleaned\n",
    "        return x\n",
    "    trimmed_df[col] = trimmed_df[col].apply(apply_trim)\n",
    "\n",
    "# Export mapping CSV\n",
    "mapping_df = pd.DataFrame(mappings, columns=[\"raw_label\", \"clean_label\"])\n",
    "mapping_df.to_csv(\"data/label_mappings/grammar_trim.csv\", index=False)\n",
    "print(f\"Exported grammar_trim mapping for {len(mapping_df)} changed rows to data/label_mappings/grammar_trim.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Part 3 Remove rare or problematic labels / options, combine similar labels together, decide threshold for what labels show up too little once all other cleaning is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
