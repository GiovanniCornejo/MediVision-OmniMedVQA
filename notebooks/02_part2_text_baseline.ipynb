{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kausthubhkonuru/School/Pattern Recognition/MediVision-Flare25/medivision/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.data import load_omnimed_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "from transformers import DataCollatorForMultipleChoice\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 42380\n",
      "Validation size: 7472\n",
      "Test size: 5535\n",
      "Overlap train-test: 0\n",
      "Overlap train-val: 0\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = load_omnimed_dataset()\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "# Check for image overlap\n",
    "print(\"Overlap train-test:\", len(set(train_df['image_path']) & set(test_df['image_path'])))\n",
    "print(\"Overlap train-val:\", len(set(train_df['image_path']) & set(val_df['image_path'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76efeff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb7efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad9ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_hf_dataset(df:pd.DataFrame):\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".csv\", delete=False) as tmp:\n",
    "        df.to_csv(tmp.name, index=False)\n",
    "        dataset = load_dataset('csv', data_files={'data': tmp.name}, split='data')\n",
    "    return dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    option_cols = [\"option_A\", \"option_B\", \"option_C\", \"option_D\"]\n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        question = str(examples[\"question\"][i])\n",
    "        options = [str(examples[col][i]) for col in option_cols]\n",
    "        first_sentences.extend([question] * 4)\n",
    "        second_sentences.extend(options)\n",
    "        label = option_cols.index(str(examples[\"gt_label\"][i]))\n",
    "        labels.append(label)\n",
    "\n",
    "    tokenized = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    result = {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized.items()}\n",
    "    #use tokenzier.decode to ensure the results are correct and no data leakage\n",
    "    \n",
    "    \n",
    "    result[\"labels\"] = labels\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc5eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data split: 5535 examples [00:00, 405432.54 examples/s]\n",
      "Generating data split: 42380 examples [00:00, 566994.90 examples/s]\n",
      "Generating data split: 7472 examples [00:00, 339587.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_test = to_hf_dataset(test_df)\n",
    "hf_train = to_hf_dataset(train_df)\n",
    "hf_val = to_hf_dataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709e415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/42380 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 42380/42380 [00:02<00:00, 20746.25 examples/s]\n",
      "Map: 100%|██████████| 7472/7472 [00:00<00:00, 20617.18 examples/s]\n",
      "Map: 100%|██████████| 5535/5535 [00:00<00:00, 18671.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "remove_columns = [\"dataset\",\"question_id\",\"modality_type\",\"question_type\",\"image_path\",\"option_A\",\"option_B\",\"option_C\",\"option_D\",\"gt_label\"]\n",
    "tokenized_train = hf_train.map(preprocess_function, batched=True,remove_columns=remove_columns)\n",
    "tokenized_val = hf_val.map(preprocess_function, batched=True,remove_columns=remove_columns)\n",
    "tokenized_test = hf_test.map(preprocess_function, batched=True,remove_columns=remove_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b052da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b948dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c82a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_text_baseline\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7983b",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d52cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/bert_text_baseline/checkpoint-2649\")\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"models/bert_text_baseline/checkpoint-2649\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5277856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def infere_and_evaluate(test_df,model,tokenizer,device):\n",
    "    #Not bached but one at a time\n",
    "    preds = []\n",
    "    gt = []\n",
    "    for idx, row in test_df.iterrows():\n",
    "        question = str(row[\"question\"])\n",
    "        options = [str(row[col]) for col in [\"option_A\", \"option_B\", \"option_C\", \"option_D\"]]\n",
    "        gt_label = row[\"gt_label\"]\n",
    "        \n",
    "        inputs = tokenizer([[question,options[0]], [question,options[1]],[question,options[2]],[question,options[3]]], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()})\n",
    "            logits = outputs.logits\n",
    "            pred = logits.argmax(dim=1).cpu().item()\n",
    "\n",
    "        preds.append(pred)\n",
    "        #gt is the index of the correct option\n",
    "        gt.append([\"option_A\", \"option_B\", \"option_C\", \"option_D\"].index(gt_label))\n",
    "\n",
    "    accuracy = accuracy_score(gt, preds)\n",
    "    precision = precision_score(gt, preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(gt, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(gt, preds, average='macro', zero_division=0)\n",
    "    results = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1\n",
    "    }\n",
    "\n",
    "    return results, preds, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7c36e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9906\n",
      "Test precision (macro): 0.9909\n",
      "Test recall (macro): 0.9913\n",
      "Test F1 (macro): 0.9911\n"
     ]
    }
   ],
   "source": [
    "results, preds,gt = infere_and_evaluate(test_df, model, tokenizer, device)\n",
    "print(f\"Test accuracy: {results[\"accuracy\"]:.4f}\")\n",
    "print(f\"Test precision (macro): {results[\"precision_macro\"]:.4f}\")\n",
    "print(f\"Test recall (macro): {results[\"recall_macro\"]:.4f}\")\n",
    "print(f\"Test F1 (macro): {results[\"f1_macro\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623ca99",
   "metadata": {},
   "source": [
    "Accuracy: 0.9906\n",
    "Precision: 0.9909\n",
    "Recall: 0.9913\n",
    "F1-Score: 0.9911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6bd7c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique questions appearing in both train and test sets: 1868\n",
      "Total number of unique questions in test set: 2316\n"
     ]
    }
   ],
   "source": [
    "unique_questions_test_list = test_df['question'].unique()\n",
    "unique_questions_train_list = train_df['question'].unique()\n",
    "questions_both = set(unique_questions_test_list) & set(unique_questions_train_list)\n",
    "print(\"Number of unique questions appearing in both train and test sets:\", len(questions_both))\n",
    "print(\"Total number of unique questions in test set:\", len(unique_questions_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "231a6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"preds\"] = preds\n",
    "test_df[\"preds\"] = test_df[\"preds\"].map({0:\"option_A\",1:\"option_B\",2:\"option_C\",3:\"option_D\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f15aea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on questions appearing in both train and test sets: 0.9916881060755987\n"
     ]
    }
   ],
   "source": [
    "test_both = test_df[test_df['question'].isin(questions_both)]\n",
    "accuracy_both = accuracy_score(test_both['gt_label'].map({\"option_A\":0,\"option_B\":1,\"option_C\":2,\"option_D\":3}), test_both['preds'].map({\"option_A\":0,\"option_B\":1,\"option_C\":2,\"option_D\":3}))\n",
    "print(\"Accuracy on questions appearing in both train and test sets:\", accuracy_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32ec5914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on questions appearing only in the test set: 0.979253112033195\n"
     ]
    }
   ],
   "source": [
    "test_unique = test_df[~test_df['question'].isin(questions_both)]\n",
    "accuracy_unique = accuracy_score(test_unique['gt_label'].map({\"option_A\":0,\"option_B\":1,\"option_C\":2,\"option_D\":3}), test_unique['preds'].map({\"option_A\":0,\"option_B\":1,\"option_C\":2,\"option_D\":3}))\n",
    "print(\"Accuracy on questions appearing only in the test set:\", accuracy_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69fad0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medivision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
